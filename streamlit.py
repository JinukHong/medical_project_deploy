from glob import glob
import time
import cv2
import numpy as np
from PIL import Image, ImageOps
import torch
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2
from ultralytics import YOLO

from module.utils import crop_preprocess
from module.model import ResNet50

import streamlit as st

import os
import gdown

def download_if_not_exists(file_path, gdrive_id):
    if not os.path.exists(file_path):
        url = f"https://drive.google.com/uc?id={gdrive_id}"
        gdown.download(url, file_path, quiet=False)


@st.cache_data
def get_model():
    # ë‹¤ìš´ë¡œë“œ ê²½ë¡œì™€ ID ì§€ì •
    resnet_path = "ckpt/ResNet50_v0.pth"
    yolo_path = "yolov8n-pose.pt"
    
    download_if_not_exists(resnet_path, "130N1bBrYHrXtJao6Jq2JlQA6RgVS3jgx")
    download_if_not_exists(yolo_path, "1htemHZjg3kYa98tFZusBOGb2CbYPG_YR")

    keypoint_model = YOLO(yolo_path)
    model = ResNet50()
    ckpt = torch.load(resnet_path, map_location='cpu')
    model.load_state_dict(ckpt["model_state_dict"])
    
    return keypoint_model, model

st.title("ì²™ì¶”ì¸¡ë§Œì¦ ì˜ˆì¸¡")

img_byte = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”", type=["jpg", "png", "jpeg"])

if img_byte is not None:
    img = Image.open(img_byte).convert('RGB')
    img = ImageOps.exif_transpose(img)
    img = np.array(img)

    st.image(
        img,
        caption=f"image shape: {img.shape[0:2]}",
        use_container_width=True,  # ìˆ˜ì •ë¨
    )

    k_model, model = get_model()

    test_aug = A.Compose([
        A.Normalize(),
        A.Resize(384, 384),
        ToTensorV2()
    ])

    start = time.time()

    crop_img = crop_preprocess(img, k_model)

    # ----- ğŸ’¡ ì˜ˆì™¸ì²˜ë¦¬: ì±„ë„ ìˆ˜ í™•ì¸ ë° ë³´ì • -----
    if crop_img is None:
        st.error("ì´ë¯¸ì§€ ì „ì²˜ë¦¬(crop_preprocess) ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤.")
        st.stop()

    if len(crop_img.shape) == 2:  # Grayscale
        crop_img = cv2.cvtColor(crop_img, cv2.COLOR_GRAY2RGB)
    elif crop_img.shape[2] == 4:  # RGBA
        crop_img = cv2.cvtColor(crop_img, cv2.COLOR_RGBA2RGB)
    elif crop_img.shape[2] != 3:
        st.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ì±„ë„ í˜•ì‹ì…ë‹ˆë‹¤: {crop_img.shape}")
        st.stop()

    test_img = test_aug(image=crop_img)["image"].unsqueeze(0)

    model.eval()
    with torch.no_grad():
        result = model(test_img)
        result = result.squeeze(0).cpu().numpy()[0]

    end = time.time()

    st.text(f"ì¶”ë¡  ì‹œê°„(cpu): {end - start:.3f} sec")
    st.text(f"ì²™ì¶” ì¸¡ë§Œì¦ì¼ í™•ë¥ : {result * 100:.3f}%")
